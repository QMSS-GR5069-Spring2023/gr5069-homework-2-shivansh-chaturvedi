{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Shivansh Chaturvedi\n",
        "GR5073 - Midterm"
      ],
      "metadata": {
        "id": "mOOrA6PiV5Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I will use KNN Classifier for this question\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# keeping only three variables + the dependent variable\n",
        "df2 = df[['capital_run_length_average:','word_freq_free:','word_freq_credit:','spam']]\n",
        "df2.head()\n",
        "\n",
        "# splitting data into y and x\n",
        "y = df2['spam']\n",
        "X = df2.loc[:, df2.columns != 'spam']"
      ],
      "metadata": {
        "id": "zbgzHnTkxZHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "print(X.shape)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "# Scaling all the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVA3QDOazFJQ",
        "outputId": "a76c0044-2999-43d7-c8ca-67d8ba14cc78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4601, 3)\n",
            "(3450, 3)\n",
            "(1151, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using GridSearchCV to tune parameters for the model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "knn_param_grid = {'n_neighbors': np.arange(1, 20, 2)}\n",
        "knngrid = GridSearchCV(KNeighborsClassifier(), param_grid=knn_param_grid)\n",
        "knngrid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters: {}\".format(knngrid.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88xPYS_RzWMx",
        "outputId": "f31ef5e4-3212-4689-cd70-bf1840251a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_neighbors': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing cross val & mean for score calculations; KFold for kfold CV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from statistics import mean \n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=11)\n",
        "knn.fit(X_train, y_train)\n",
        "kfold = KFold()\n",
        "\n",
        "print(\"KNN CLASSIFER (SCALED) SCORES\")\n",
        "print(\"Test Set Score: {:.3f}\".format(knn.score(X_test, y_test)))\n",
        "print(\"Mean Cross Validation Score using KFold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train, cv=kfold))))\n",
        "knn_test_score = knn.score(X_test, y_test)\n",
        "knn_mean_score = np.mean(cross_val_score(knn, X_train, y_train, cv=kfold))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"I used GridSearchCV to decide on a specific value of K: this value came out to be n=11.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHLFV0UpDxGh",
        "outputId": "59cc26e1-7bfa-468d-c0d3-dcb236d5526f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN CLASSIFER (SCALED) SCORES\n",
            "Test Set Score: 0.808\n",
            "Mean Cross Validation Score using KFold: 0.817\n",
            "\n",
            "\n",
            "I used GridSearchCV to decide on a specific value of K: this value came out to be n=11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (9) Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
      ],
      "metadata": {
        "id": "pek0ctHVErhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I will use random forest classifier for this question\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_param_grid = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 10), 'n_estimators':np.arange(1, 10)}\n",
        "forestgrid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=forest_param_grid)\n",
        "forestgrid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters: {}\".format(forestgrid.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4H7D04FEwlz",
        "outputId": "f9a3bf74-7fec-47b8-a827-7bf94e5e6ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'criterion': 'gini', 'max_depth': 6, 'n_estimators': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forest = RandomForestClassifier(criterion = 'gini', max_depth = 6, n_estimators = 9, random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "print(\"RANDOM FOREST (SCALED) SCORES\")\n",
        "print(\"Test set score: {:.3f}\".format(forest.score(X_test, y_test)))\n",
        "print(\"Mean Cross Validation Score using KFold: {:.3f}\".format(np.mean(cross_val_score(forest, X_train, y_train, cv=kfold))))\n",
        "forest_test_score = forest.score(X_test, y_test)\n",
        "forest_mean_score = np.mean(cross_val_score(forest, X_train, y_train, cv=kfold))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"I used GridSearchCV to decide on some of the parameters for RandomForestClassifier (I did not do all possible parameters as it was computationally expensive) and found that.\")\n",
        "print(\"the best results were at 'criterion': 'gini', 'max_depth': 6, 'n_estimators': 9\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Other Model Results So Far\")\n",
        "print(\"KNN Test Set Score: {:.3f}\".format(knn_test_score))\n",
        "print(\"KNN Mean CV Score: {:.3f}\".format(knn_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The Random Forest model was better than the KNNClassifier Model for both Mean Score and Test Score.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChDP6APBwvbJ",
        "outputId": "061fbbb6-3fc6-43b3-b7cd-10dabea1ebf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RANDOM FOREST (SCALED) SCORES\n",
            "Test set score: 0.810\n",
            "Mean Cross Validation Score using KFold: 0.826\n",
            "\n",
            "\n",
            "I used GridSearchCV to decide on some of the parameters for RandomForestClassifier (I did not do all possible parameters as it was computationally expensive) and found that.\n",
            "the best results were at 'criterion': 'gini', 'max_depth': 6, 'n_estimators': 9\n",
            "\n",
            "\n",
            "Other Model Results So Far\n",
            "KNN Test Set Score: 0.808\n",
            "KNN Mean CV Score: 0.817\n",
            "\n",
            "\n",
            "The Random Forest model was better than the KNNClassifier Model for both Mean Score and Test Score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (10) Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?\n",
        "\n"
      ],
      "metadata": {
        "id": "YAMSS2tSXMCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I will use logistic regression for this question\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "logreggrid = GridSearchCV(LogisticRegression(random_state=42), param_grid=logreg_param_grid)\n",
        "logreggrid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters: {}\".format(logreggrid.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8scXh8AXKzL",
        "outputId": "830f9961-69f4-4e20-e71a-db6e5b10dab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(penalty = 'none', C=100, random_state=42)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "print(\"LOGISTIC REGRESSION (SCALED) SCORES\")\n",
        "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "print(\"Mean Cross Validation Score using KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train, y_train, cv=kfold))))\n",
        "logreg_test_score = logreg.score(X_test, y_test)\n",
        "logreg_mean_score = np.mean(cross_val_score(logreg, X_train, y_train, cv=kfold))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"I used GridSearchCV to find that the best C value for an unpenalized Logistic Regression is C=100\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Other Model Results So Far\")\n",
        "print(\"KNN Test Set Score: {:.3f}\".format(knn_test_score))\n",
        "print(\"KNN Mean CV Score: {:.3f}\".format(knn_mean_score))\n",
        "print(\"Forest Test Set Score: {:.3f}\".format(forest_test_score))\n",
        "print(\"Forest Mean CV Score: {:.3f}\".format(forest_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The Logistic Regression model was not better than the Random Forest Model or than the KNN Classifier Model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFnXKtZjZGQz",
        "outputId": "0e2165cf-b54a-4d3b-cd41-64f5cc0284f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOGISTIC REGRESSION (SCALED) SCORES\n",
            "Test set score: 0.770\n",
            "Mean Cross Validation Score using KFold: 0.775\n",
            "\n",
            "\n",
            "I used GridSearchCV to find that the best C value for an unpenalized Logistic Regression is C=100\n",
            "\n",
            "\n",
            "Other Model Results So Far\n",
            "KNN Test Set Score: 0.808\n",
            "KNN Mean CV Score: 0.817\n",
            "Forest Test Set Score: 0.810\n",
            "Forest Mean CV Score: 0.826\n",
            "\n",
            "\n",
            "The Logistic Regression model was not better than the Random Forest Model or than the KNN Classifier Model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (11) Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?\n"
      ],
      "metadata": {
        "id": "KFY41emKa91N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I will use Decision Tree Classifier for this question\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_param_grid = {'criterion':['gini','entropy'],'max_depth': np.arange(1, 10), 'min_samples_leaf':np.arange(1,10)}\n",
        "treegrid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid=tree_param_grid)\n",
        "treegrid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters: {}\".format(treegrid.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOMKoXN9bCh_",
        "outputId": "c5b9abb5-1d8b-4a98-ed7b-b95e8647b6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 8, min_samples_leaf = 7, random_state=42)\n",
        "tree.fit(X_train,y_train)\n",
        "\n",
        "print(\"TREE MODEL (SCALED) SCORES\")\n",
        "print(\"Test set score: {:.3f}\".format(tree.score(X_test, y_test)))\n",
        "print(\"Mean Cross Validation Score using KFold: {:.3f}\".format(np.mean(cross_val_score(tree, X_train, y_train, cv=kfold))))\n",
        "tree_test_score = tree.score(X_test, y_test)\n",
        "tree_mean_score = np.mean(cross_val_score(tree, X_train, y_train, cv=kfold))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"I used GridSearchCV to find that the best parameters for this Tree Model are 'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 7\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Other Model Results So Far\")\n",
        "print(\"KNN Test Set Score: {:.3f}\".format(knn_test_score))\n",
        "print(\"KNN Mean CV Score: {:.3f}\".format(knn_mean_score))\n",
        "print(\"Forest Test Set Score: {:.3f}\".format(forest_test_score))\n",
        "print(\"Forest Mean CV Score: {:.3f}\".format(forest_mean_score))\n",
        "print(\"Logistic Regression Test Set Score: {:.3f}\".format(logreg_test_score))\n",
        "print(\"Logistic Regression Mean CV Score: {:.3f}\".format(logreg_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The Tree Model was better than the Logistic Regression.\") \n",
        "print(\"It was, however, not better than the Random Forest Model and KNN Classifier Model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da8vvlmRdv-8",
        "outputId": "b1c7b03b-c803-4184-bc6b-267d01a98ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TREE MODEL (SCALED) SCORES\n",
            "Test set score: 0.802\n",
            "Mean Cross Validation Score using KFold: 0.815\n",
            "\n",
            "\n",
            "I used GridSearchCV to find that the best parameters for this Tree Model are 'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 7\n",
            "\n",
            "\n",
            "Other Model Results So Far\n",
            "KNN Test Set Score: 0.808\n",
            "KNN Mean CV Score: 0.817\n",
            "Forest Test Set Score: 0.810\n",
            "Forest Mean CV Score: 0.826\n",
            "Logistic Regression Test Set Score: 0.770\n",
            "Logistic Regression Mean CV Score: 0.775\n",
            "\n",
            "\n",
            "The Tree Model was better than the Logistic Regression.\n",
            "It was, however, not better than the Random Forest Model and KNN Classifier Model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (12) Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models?  \n",
        "\n"
      ],
      "metadata": {
        "id": "SR97RXCQgwOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I will use Random Forest Classifier for this question, as it gave me the best Test Set Score above\n",
        "\n",
        "# having six variables total + the dependent variable (I added variables for 'receive', 'money' and '!')\n",
        "df3 = df[['capital_run_length_average:','word_freq_free:','word_freq_credit:', 'word_freq_receive:', 'word_freq_money:', 'char_freq_!:', 'spam']]\n",
        "\n",
        "# splitting data into y and x\n",
        "y2 = df3['spam']\n",
        "X2 = df3.loc[:, df3.columns != 'spam']"
      ],
      "metadata": {
        "id": "R22-tSYagyOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train2)\n",
        "X_train2 = scaler.transform(X_train2)\n",
        "X_test2 = scaler.transform(X_test2)"
      ],
      "metadata": {
        "id": "LiIweqOQh8KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest.fit(X_train2, y_train2)\n",
        "\n",
        "print(\"RANDOM FOREST (SCALED) SCORES WITH 6 VARIABLES\")\n",
        "print(\"Test set score: {:.3f}\".format(forest.score(X_test2, y_test2)))\n",
        "print(\"Mean Cross Validation Score using KFold: {:.3f}\".format(np.mean(cross_val_score(forest, X_train2, y_train2, cv=kfold))))\n",
        "forest2_test_score = forest.score(X_test2, y_test2)\n",
        "forest2_mean_score = np.mean(cross_val_score(forest, X_train2, y_train2, cv=kfold))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"RESULTS FOR MODELS WITH 3 VARIABLES\")\n",
        "print(\"KNN Test Set Score: {:.3f}\".format(knn_test_score))\n",
        "print(\"KNN Mean CV Score: {:.3f}\".format(knn_mean_score))\n",
        "print(\"Forest Test Set Score: {:.3f}\".format(forest_test_score))\n",
        "print(\"Forest Mean CV Score: {:.3f}\".format(forest_mean_score))\n",
        "print(\"Logistic Regression Test Set Score: {:.3f}\".format(logreg_test_score))\n",
        "print(\"Logistic Regression Mean CV Score: {:.3f}\".format(logreg_mean_score))\n",
        "print(\"Tree Test Set Score: {:.3f}\".format(tree_test_score))\n",
        "print(\"Tree Mean CV Score: {:.3f}\".format(tree_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"This new Random Forest model with 6 variables was a better predictor of test data than all the models before.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UBEcRDUiLmo",
        "outputId": "f3c090e1-3d51-46b1-898b-c066669597cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RANDOM FOREST (SCALED) SCORES WITH 6 VARIABLES\n",
            "Test set score: 0.892\n",
            "Mean Cross Validation Score using KFold: 0.872\n",
            "\n",
            "\n",
            "RESULTS FOR MODELS WITH 3 VARIABLES\n",
            "KNN Test Set Score: 0.808\n",
            "KNN Mean CV Score: 0.817\n",
            "Forest Test Set Score: 0.810\n",
            "Forest Mean CV Score: 0.826\n",
            "Logistic Regression Test Set Score: 0.770\n",
            "Logistic Regression Mean CV Score: 0.775\n",
            "Tree Test Set Score: 0.802\n",
            "Tree Mean CV Score: 0.815\n",
            "\n",
            "\n",
            "This new Random Forest model with 6 variables was a better predictor of test data than all the models before.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (13) Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  "
      ],
      "metadata": {
        "id": "X3bWOj7ykW9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K Neighbors Classifier\n",
        "\n",
        "knn.fit(X_train2, y_train2)\n",
        "\n",
        "print(\"KNN CLASSIFER (SCALED) SCORES\")\n",
        "print(\"Test Set Score with 6 variables: {:.3f}\".format(knn.score(X_test2, y_test2)))\n",
        "print(\"Mean Cross Validation Score using KFold with 6 variables: {:.3f}\".format(np.mean(cross_val_score(knn, X_train2, y_train2, cv=kfold))))\n",
        "print(\"Test Set Score with 3 variables: {:.3f}\".format(knn_test_score))\n",
        "print(\"Mean Cross Validation Score using KFold with 3 variables: {:.3f}\".format(knn_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "logreg.fit(X_train2, y_train2)\n",
        "\n",
        "print(\"LOGISTIC REGRESSION (SCALED) SCORES\")\n",
        "print(\"Test Set Score with 6 variables: {:.3f}\".format(logreg.score(X_test2, y_test2)))\n",
        "print(\"Mean Cross Validation Score using KFold with 6 variables: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train2, y_train2, cv=kfold))))\n",
        "print(\"Test Set Score with 3 variables: {:.3f}\".format(logreg_test_score))\n",
        "print(\"Mean Cross Validation Score using KFold with 3 variables: {:.3f}\".format(logreg_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Tree\n",
        "\n",
        "tree.fit(X_train2, y_train2)\n",
        "\n",
        "print(\"TREE MODEL (SCALED) SCORES\")\n",
        "print(\"Test Set Score with 6 variables: {:.3f}\".format(tree.score(X_test2, y_test2)))\n",
        "print(\"Mean Cross Validation Score using KFold with 6 variables: {:.3f}\".format(np.mean(cross_val_score(tree, X_train2, y_train2, cv=kfold))))\n",
        "print(\"Test Set Score with 3 variables: {:.3f}\".format(tree_test_score))\n",
        "print(\"Mean Cross Validation Score using KFold with 3 variables: {:.3f}\".format(tree_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"RANDOM FOREST (SCALED) SCORES\")\n",
        "print(\"Test set score with 6 variables: {:.3f}\".format(forest2_test_score))\n",
        "print(\"Mean Cross Validation Score using KFold with 6 variables: {:.3f}\".format(forest2_mean_score))\n",
        "print(\"Test set score with 3 variables: {:.3f}\".format(forest_test_score))\n",
        "print(\"Mean Cross Validation Score using KFold with 3 variables: {:.3f}\".format(forest_mean_score))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Summarizing\n",
        "\n",
        "print(\"Based on the scores above, as well as the scores from the previous question, the Random Forest Model with 6 Variables\")\n",
        "print(\"had the highest test score and the highest Mean Cross Val of all the models presented so far (in this random state). \")\n",
        "print(\"Based on this performance, I would choose this model, as it is the best predictor of unseen data. \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmwdSPEHkWtt",
        "outputId": "ab382a56-29cb-4034-8956-d4b9e976249f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN CLASSIFER (SCALED) SCORES\n",
            "Test Set Score with 6 variables: 0.862\n",
            "Mean Cross Validation Score using KFold with 6 variables: 0.860\n",
            "Test Set Score with 3 variables: 0.808\n",
            "Mean Cross Validation Score using KFold with 3 variables: 0.817\n",
            "\n",
            "\n",
            "LOGISTIC REGRESSION (SCALED) SCORES\n",
            "Test Set Score with 6 variables: 0.823\n",
            "Mean Cross Validation Score using KFold with 6 variables: 0.818\n",
            "Test Set Score with 3 variables: 0.770\n",
            "Mean Cross Validation Score using KFold with 3 variables: 0.775\n",
            "\n",
            "\n",
            "TREE MODEL (SCALED) SCORES\n",
            "Test Set Score with 6 variables: 0.878\n",
            "Mean Cross Validation Score using KFold with 6 variables: 0.858\n",
            "Test Set Score with 3 variables: 0.802\n",
            "Mean Cross Validation Score using KFold with 3 variables: 0.815\n",
            "\n",
            "\n",
            "RANDOM FOREST (SCALED) SCORES\n",
            "Test set score with 6 variables: 0.892\n",
            "Mean Cross Validation Score using KFold with 6 variables: 0.872\n",
            "Test set score with 3 variables: 0.810\n",
            "Mean Cross Validation Score using KFold with 3 variables: 0.826\n",
            "\n",
            "\n",
            "Based on the scores above, as well as the scores from the previous question, the Random Forest Model with 6 Variables\n",
            "had the highest test score and the highest Mean Cross Val of all the models presented so far (in this random state). \n",
            "Based on this performance, I would choose this model, as it is the best predictor of unseen data. \n"
          ]
        }
      ]
    }
  ]
}